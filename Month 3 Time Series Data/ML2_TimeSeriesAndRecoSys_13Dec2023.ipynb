{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content"
      ],
      "metadata": {
        "id": "NQn5v4H5k7Qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MF for Clustering\n",
        "- Non Negative MF\n",
        "- Bulding a Rec sys with multiple data matrices\n",
        "- Netflix Prize Solution\n"
      ],
      "metadata": {
        "id": "gnj-3HzHllNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://docs.google.com/uc?id=1HzBTlKPEr2Vho_ViABwPYi7Ih-UOss24)"
      ],
      "metadata": {
        "id": "W418m1rp4Qkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Matrix Factorization for Clustering"
      ],
      "metadata": {
        "id": "K7QIssZ8lH0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is also a special case of MF\n",
        "\n",
        "Lets consider K-Means clustering, where the dataset is just a bunch of $x_i$'s (let that be n), and there is no label (y).\n",
        "\n",
        "> **Recall the underlying optimization problem of K-Means**\n",
        "\n",
        "We wish to find cluster centroids $C_j$'s (1 to k) such that, when we summate over the k clusters, for each $x_i$ belonging to the cluster, we get a minimal distance (euclidean, or any other distance).\n",
        "\n",
        "We're essentially trying to minimise the **intra cluster distances**.\n",
        "\n",
        "Let $S_j$ be the set of points in cluster j.\n",
        "\n",
        "$min_{C_j} Σ_{j=1}^k Σ_{x_i ϵ S_j} ||x_i - C_j||^2 $\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1WP4k8jVcpM7KZ2rJYsLrgMKUN0CJT5cp)\n",
        "\n",
        "> **Q. Can we transform this optimisation problem to a matrix form?**\n",
        "\n",
        "Transforming this to matrix form.\n",
        "\n",
        "Let's define a matrix $Z$ such that\n",
        "- $Z_{ij} = 1$ if $x_i ϵ S_j$\n",
        "- $Z_{ij} = 0$ otherwise\n",
        "- Dimensions become `n x k`\n",
        " - where, n-> No of data points; <br>k-> No of clusters\n",
        "\n",
        "Essentially, $Z_{ij}$ tells if ith point belongs to jth cluster. It is known as the **Cluster Assignment Matrix**.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1FdF5XS32Q69H9HtYiAgsDwYrbuDE8hWM)\n",
        "\n",
        "Our optimization problem can be approximated to the following:-\n",
        "\n",
        "$min_{C_j, Z_{ij}} Σ_{i=1}^n Z_{ij}||x_i - C_j||^2 $\n",
        "\n",
        "The logic behind this is that\n",
        "- whenever $Z_{ij} = 1$, we'll take distance between point $x_i$ and cluster centroid $C_j$ into consideration.\n",
        "- whereas when $Z_{ij} = 0$, then we do not need to consider the distance.\n",
        "\n",
        "But naturally, we'd have to impose a few **constraints** in order to make this assumption:-\n",
        "- Value of $Z_{ij}$ can be 0 or 1 only.\n",
        "- A data point $x_i$ should be assigned to a single cluster centroid only. Therefore, the summation of a row i, needs to be 1, i.e. $Σ_{j=1}^k Z_{ij} = 1$\n",
        "\n",
        "Also, let's define:-\n",
        "- Matrix $X_{nxd}$: Here, the data points $x_i$'s are stacked as rows.\n",
        "- Matrix $C_{dxk}$: Here, the cluster centroids are stacked along columns.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=19L7G_CmG3T8ShzfKJVzGBgGF5uXc0_dO)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aRn3YWRvlN4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **NOTE:**\n",
        "- After matrix manipulation, we can prove our result, but we're skipping it cause it'll be long, and directly giving the solution.\n",
        "\n",
        "We need to somehow use these 3 matrices: `Z, X and C` to convert our optimisation problem to matrix form. After matrix manipulation, We can write it as:-\n",
        "\n",
        "$min_{C, Z} ||X_{nxd} - Z_{nxk}.C_{kxd}^T||_F^2$\n",
        "\n",
        "Here, the `F` in suvscript represents **Forbenius norm**.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What is Frobenius norm?**\n",
        "\n",
        "Recall that a **square norm** of a vector means taking square of all the elements in the vector.\n",
        "\n",
        "When we expand this concept to matrix, it's called **Forbenius norm**. Mathematically, Forbenius norm for a matrix X can be expressed as:-\n",
        "\n",
        "$||X||_F = Σ_{i=1}^{rows} Σ_{j=1}^{col} X_{ij}^2 $\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=17Og-bbcicyWlHH4BoHQ4Wui7SHKw2i_W)\n",
        "\n",
        "Hence, effectively in K-Means clustering, we're just trying to decompose our $X$ matrix as product of two matrices,\n",
        "\n",
        "i.e. $X ≈Z.C^T$\n",
        "\n",
        "Where, as we saw,\n",
        "- $Z_{nxk}$: Cluster Allocation Matrix\n",
        "- $C_{kxd}$: Cluster Centroid Matrix\n",
        "\n",
        "Therefore, we can call KMeans clustering as a special case of Matrix factorization."
      ],
      "metadata": {
        "id": "EisE7OUSmzQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. Can we also call Gaussian Mixture Model as a special case of MF?\n",
        "Yes.\n",
        "\n",
        "Essentially, GMM is just clustering with **soft assignment**.\n",
        "\n",
        "So, similar to clustering with a hard assignment (KMeans), that we just saw, we can also prove GMM to be a case of MF.\n",
        "\n",
        "The only difference is that the constraint we saw on Z matrix that $Z_{ij}$ must be 0 or 1, will no longer be applicable.\n",
        "\n",
        "However, the sencond constraint still holds, i.e. $Σ_{j=1}^k Z_{ij} = 1$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=16hT54jP8RtidLxZaf7_6hEZlPJ5pf_SL)"
      ],
      "metadata": {
        "id": "xi82tpGRs2yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Takeaway:\n",
        "\n",
        "We saw that MF is connected to so many concepts that we've already studied:-\n",
        "- Matrix Completion, and therefore Recommender systems\n",
        "- PCA, SVD\n",
        "- K Means (hard assignment)\n",
        "- GMM (soft assignment)\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1LkVu7VY9e1q-qeyiAvYy80aWBWwI7Q6a)"
      ],
      "metadata": {
        "id": "bSGwgFMSuivi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Hyperparameter tuning: How to find the value of d in MF for Recommender Systems?\n"
      ],
      "metadata": {
        "id": "s2yaK1bj0hcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the Recommender System setup (which is essentially Matrix completion).\n",
        "\n",
        "Given a sparse matrix $A_{nxm}$ which consists of ratings given by n users to m items, we were supposed to decompose it into product of two matrices to represent user and item matrices, with dimensions `n x d` and `m x d` respectively.\n",
        "\n",
        "$A_{nxm} = B_{nxd}.C_{dxm}^T$\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Given this relation, what is the optimization problem?**\n",
        "\n",
        "We can re-write this relation as our optimization problem as:-\n",
        "\n",
        "$min_{B_i, C_j} Σ (A_{ij} - B_i^T . C_j)^2$\n",
        "\n",
        "Where, we're only minimising that for all the cells that are **non-empty** in A.\n",
        "\n",
        "Now that we know about the Frobenius norm notation, we can express our optimization problem as $min_{B, C} ||A - B.C^T||_F^2$ also.\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1BlOsR3WXMYQnF0JPoy9Q5VEktPIjAMRr)\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "rAhr55M00qkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Q. How can we find d?**\n",
        "\n",
        "Just like `k` in Clustering, `d` is also a **hyper parameter**.\n",
        "\n",
        "`d` is not something that we're \"optimising for\". We're only optimizing for $B_i$ and $C_j$ vectors.\n",
        "\n",
        "Let's call the optimisation problem, i.e. $min_{B_i, C_j} Σ (A_{ij} - B_i^T . C_j)^2$ as **loss**.\n",
        "\n",
        "We need to try various values of d, to see how loss is affected. In fact, let's plot **loss vs. d**.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Since we need to plot, Are there any contraints to value of `d`?**\n",
        "\n",
        "- Value of `d` should obviously be a positive number.\n",
        " - i.e, $d>0$\n",
        "- Naturally, we want d to be less than number of users (n) and number of items (m).\n",
        " - This can be written as $d<=min(n,m)$\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How would an increasing value of `d` affect the loss?**\n",
        "\n",
        "Intuitively,\n",
        "- As value of `d` increases, our vectors $B_{i_{1xd}}$ and $C_{j_{1xd}}$ carry more information/parameters, as the vector length would increase.\n",
        "- This means that their products would be more close to original matrix $A$.\n",
        "- Hence, the loss would **decrease**.\n",
        "\n",
        "<bR>\n",
        "\n",
        "> **Q. How can we utilise this curve to find the optimal value of `d`?**\n",
        "\n",
        "We can choose the **elbow point / inflection point**, and choose that value of `d` as the most optimal value.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Is there any disadvantage of using this technique?**\n",
        "\n",
        "Yes.\n",
        "\n",
        "However this technique is used in practice, in some places.\n",
        "\n",
        "On choosing the elbow point as the optimal value for `d`, there is a chance that we might **overfit**.\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1D_1J-TuoZbRaL9P1iu3Cvt2-6bs--Rop)"
      ],
      "metadata": {
        "id": "_w2xY19pmYtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Q. How else can we solve for `d` without risking overfitting?**\n",
        "\n",
        "We're not trying to solve a clustering problem. We do have the ground truth data (i.e. actual ratings from users given to different items) stored in matrix A.\n",
        "\n",
        "Let's utilise this and perform a proper **hyper parameter tuning** of `d`.\n",
        "\n",
        "Let's **train** our model using 80% of the non-empty values in $A$ in random, to find the values for matrix $B$ and $C$. We can then **test** these on the remaining 20%, to see how well a particular value of `d` is performing. This can be again done by:-\n",
        "- Stochastic Gradient Descent\n",
        "- Coordinate Descent Algo\n",
        "\n",
        "Clearly, this is a better approach than using elbow point.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1GGLQgYjUDSg1ojklPiQniKcXp4pZwWFj)"
      ],
      "metadata": {
        "id": "l71VsqyegxTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. What is the interpretability behind value of d? What does it signify?\n",
        "\n",
        "There is **NO** interpretability behind value of d. It just an **embedding** that has no meaning in the real world.\n",
        "\n",
        "So when we factorise $A_{ij}$ as product of user vector $B_i / U_i$ and item vector $C_j / I_j$, there is no significant meaning of an element in either vector. They're just numerical values.\n",
        "\n",
        "However, using the concept of **UMAP/t-SNE** that we've studied,\n",
        "- we can represent these d-dimensional user vectors into 2-dimensional,\n",
        "- so that we can project this data. Hence, each point becomes a user.\n",
        "- Once we have this projection, we can use it to **cluster similar users**.\n",
        "\n",
        "This interpretability we can get after UMAP/t-SNE, but on their own, these have no meaning.\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=19tZoqlYrglEs6rJBsJj32AvqerFeI5sc)"
      ],
      "metadata": {
        "id": "Grw_-OM2mZ-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Non Negative Matrix Factorization (NMF)"
      ],
      "metadata": {
        "id": "dfiIy763eWlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the **MF problem statement**. Given a matrix $A$, we have to decompose it into the product of two matrices $B$ and $C$.\n",
        "\n",
        "$A_{nxm} = B_{nxd}.C_{dxm}$\n",
        "\n",
        "In certain situations, we might want the elements of $B_{nxd}$ and/or $C_{dxm}$ to be non-negative, i.e. $B_{ij}>= 0$ and/or $C_{ij} >= 0$\n",
        "\n",
        "This technique is called **Non Negative Matrix Factorization**.\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1DSnQMXjnUIVTrO8qAKfHiYjg-HZ5qKEX)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Why would we want non-negative elements in the MF factors?**\n",
        "\n",
        "The following scenarios demand that the matrix factors have non-negative elements:-\n",
        "- **MF for Clustering**: ($X_{nxd} = Z_{nxk}^T.C_{kxd}$)\n",
        " - Recall the contraints on $Z_{nxk}$, **cluster assignment matrix.**\n",
        " - One constraint during hard assignment was that <br>\n",
        " $Z_{ij} = 1$, if data point i belongs to cluster j, i.e. $x_i ϵ S_j$, <br>\n",
        " $Z_{ij} = 0$, otherwise\n",
        " - So essentially, there is an **implicit constraint** that all values in $Z$ be **non-negative**, i.e. be $>=0$\n",
        " - **Note:** Here the contraint of being non-negative is only on one matrix, i.e. $Z$. <br>There is no such constraint on $C$, values there can be negative also.\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1Qrvs7A4dsfGRLBS6yMoOUNxH60e2TpGc)\n",
        "\n",
        "\n",
        "Hence, NMF can be regarded as the generalization of Clustering, which is itself a special case of MF. $MF \\supseteq NMF \\supseteq K-Means$\n",
        "\n",
        "- **Visualising images**\n",
        " - We know images are stored as RGB values of each pixel, that range from 0 to 255.\n",
        " - In implementation, we sometimes scale these values to the range of 0 to 1.\n",
        " - These values are hence, non-negative.\n",
        " - We will see shortly how to express this problem in form of matrices when we cover **Eigen faces** concept.\n",
        "\n",
        "So, essentially, NMF is helping us to **interpret the results better**.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1RxL-Hvj-oRbbj83IRbEu8i0--IVeycLA)\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=19lEbqHQh-oPcFlFRwv54TDQFr2Ky6b0j)\n",
        "\n"
      ],
      "metadata": {
        "id": "WzZSsVvOebeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, NMF is not always neccessary.\n",
        "\n",
        "When we're doing MF for our Recommender systems, we don't need it as we don't care what our $B_i$ or $C_j$ is, we only care that the product gives a close approximation of ratings matrix $A$.\n"
      ],
      "metadata": {
        "id": "OjrSWyG3t4xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Building a Rec Sys with multiple data matrices\n",
        "### Suppose you're a data scientist at Youtube, and you have three separate matrices:-\n",
        "- $L_{nxm}$: Containing data about videos that users have liked\n",
        "- $W_{nxm}$: Containing data about videos that users have watched more than 50%\n",
        "- $D_{nxm}$: Containing data about videos that users have disliked\n",
        "\n",
        "Given all these matrices, we wish to recommend new videos to a user $u_i$.\n",
        "\n",
        "Now how will we build a Recommender System?\n",
        "\n",
        "> **INTERACT**\n"
      ],
      "metadata": {
        "id": "JUQd_6M6WzG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Idea 1\n",
        "> **Q. What if we simply add the three matrices, and then perform Matrix completion using MF?**\n",
        "\n",
        "This doesn't make sense.\n",
        "\n",
        "There is no point in adding two matrices containing data about likes and dislikes. They are completely orthogonal/opposite signals."
      ],
      "metadata": {
        "id": "8T3G-GGjXpgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Idea 2\n",
        "\n",
        " > **Q. Instead of simply adding, what if we assign some positive or negative weights on these matrices, depending on their effect?**\n",
        "\n",
        "This is a valid approach.\n",
        "\n",
        "The idea here is to combine the three matrices into one, using appropriate weights, and then perform MF as we normally would.\n",
        "\n",
        "$α_1 L_{nxm} + α_2 W_{nxm} + α_3 D_{nxm} = A_{nxm}$\n",
        "\n",
        "For instance\n",
        "- $α_1$ would have a strong positive weight `1`\n",
        "- $α_2$ would be a smaller positive weight, say `0.1`\n",
        "- $α_3$ would have a strong negative weight, say `-1`\n",
        "\n",
        "Naturally, with this approach, we'll have the **challenge** to find these optimal weights.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1pUieykwbbLLNC130naiG-_v3UmL1pqoE)\n",
        "\n"
      ],
      "metadata": {
        "id": "8t6nPAMRbTak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Idea 3\n",
        "\n",
        " > **Q. What if we perform  independent factorization on these matrices?**\n",
        "\n",
        "Upon independant factorization, we can obtain user and item (videos) vectors from each of these three matrices:\n",
        "\n",
        "$u_i^L, u_i^W, u_i^D$ and $v_i^L, v_i^W, v_i^D$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1PwqYksQ5D-Mc61o8G__gc8YIq-2QNFmt)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Now what do we do with these user and item vectors?**\n",
        "\n",
        "These are nothing but user profiles with data about the videos that they like and dislike. Using these, we can find similar users using **cosine similarity**, and recommend videos accordingly.\n",
        "\n",
        "Similarly, similar items can also be found out using cosine similarity, and recommendations would be made accordingly.\n",
        "\n",
        "We will look at which items user $u_i$ has the highest tendency of liking, and avoid videos that the user has a tendency to dislike.\n",
        "\n",
        "We use the data given by watched matrix `W` also, but naturally, it gets a lower preference than the data in liked matrix `L`.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1QtwbIYmVJ2ob9ByIq8PKg2173woIj1k4)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "> **Q. What if we apply matrix completion?**\n",
        "\n",
        "\n",
        "Consider user $u_{100}$ and $v_{10}$. The cell represented by these in matrix L is empty, i.e. $L_{100, 10} = NULL$.\n",
        "\n",
        "Now, in order to get this data, we'll have to do\n",
        "\n",
        "$u_{100}^T.v_{10}$\n",
        "\n",
        "If $u_{100}$ and $v_{10}$ are unit vectors, then this is the same as cosine similarity. Also, they represent the user and video profiles repectively.\n",
        "\n",
        "So essentially, we will get very similar results.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1tvQe1kMRWjNR0GOxh59dnh0-OffQuxB3)"
      ],
      "metadata": {
        "id": "dTBeKPj_aJ-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Netflix Prize Solution"
      ],
      "metadata": {
        "id": "b0FmY-f-1a4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Around 2008-09, Netflix organised a world wide competition, where they invited machine learning enthusiasts of the world to try and improve their recommendation engine.\n",
        "\n",
        "An **incentive of a million dollars** was there to attract people to collaborate and try to find the solution.\n",
        "\n",
        "The loss metric to judge the recommendation engine's performance was **RMSE**, and the task presented to the world was to somehow improve the RMSE score by **10%**.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What did the final solution look like?**\n",
        "\n",
        "The final solution had tons of models (about 30-40 models with their own hyper parameters) along with bagging and boosting models.\n",
        "\n",
        "Naturally, this became quite messy, but a lot of good research work came out of it.\n",
        "\n",
        "This was so complex, that it was **never productionised**.\n",
        "\n",
        "In fact, the use of Matrix Factorization for Rec Sys, is a by-product of this competition.\n",
        "\n",
        "> External links to share\n",
        "- There is a research paper that summarises the most important ideas given by this competition. This doesn't contain the winning solution It can be found at:\n",
        " - https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\n",
        "- The winning solution can be found at:\n",
        " - https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1tZEKFbbp_zFj1SWqPLhp3QoVGTm6PMJ0)"
      ],
      "metadata": {
        "id": "Q4za9FU11dR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We encourage you to go ahead and read these research papers. Therefore, we'll keep the notations same here as we study.\n",
        "\n",
        "Let\n",
        "- $p_u$: d-dimensional representation for uth user vector. (We've been using $u_i$ so far)\n",
        "- $q_i$: d-dimensional representation for ith item vector. (We've been using $I_j/m_j$ so far)\n",
        "- $r_{ui}$: rating of movie i, given by user u\n",
        "\n",
        "<br>\n",
        "\n",
        "> **NOTE:**\n",
        "- Visit the link https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\n",
        "- And parallely show the equations of the following section in the research paper also\n",
        "\n",
        "> **Q. How can the Netflix problem be formulated?**\n",
        "\n",
        "- Given some data, we split into train ($D_{tr}$) and test data ($D_{te}$)\n",
        "\n",
        "- Simplest formulation of the problem:\n",
        "\n",
        " - Summating over all users and all items, the square of rating given by user u on movie i (i.e. $r_{ui}$), minus the dot product of d-dimensional representations of user u and movie i., we wish to minimise the $p_u$'s and $q_i$'s.\n",
        "\n",
        "$min_{p_u, q_i} Σ_{u, i} (r_{ui} - q_i^T.p_u)^2$\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1MUAdZmBSLpu_egDn-HJxiSwC__3MNvUF)\n",
        "\n",
        "There are no constraints here, since right now, we don't care about the interpretability.\n",
        "\n",
        "#### Adding regularization term\n",
        "\n",
        "- If our value for d is very high, we run the risk of overfitting on the training data.\n",
        "- Hence, the next step is to **regularize** in order to avoid an overfit.\n",
        "- Let $λ$ be regularization coefficient, so the equation becomes:-\n",
        "\n",
        "$min_{p_u, q_i} Σ_{u, i} (r_{ui} - q_i^T.p_u)^2 + λ(Σ_i ||q_i||^2 + Σ_u ||p_u||^2)$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1TTcRR7SfC5SEAeAynNTItEUQ0hoh1NHq)\n",
        "\n",
        "\n",
        "> **Now, we can apply SGD here, in order to solve.**\n",
        "\n",
        "- Error term becomes: $e_{ui} = r_{ui} - q_i^T.p_u$\n",
        "- Let the learning rate: $γ$\n",
        "\n",
        "$q_i = q_i + γ(e_{ui}.p_u - λ.q_i)$\n",
        "\n",
        "$p_u = p_u + γ(e_{ui}.q_i - λ.p_u)$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1GSsm5_1Tu5RIRmXZGwh5xNEWnUkIhCD6)"
      ],
      "metadata": {
        "id": "yjsJsKWbXvld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Adding biases\n",
        "\n",
        "> **Suppose that we want an estimate of a user Joe’s rating of the movie Titanic.**\n",
        "\n",
        "- Now, say that the average rating over all movies, µ,\n",
        "is 3.7 stars.\n",
        "- Furthermore, Titanic is better than an average movie, so it tends to be rated 0.5 stars above the average.\n",
        "- On the other hand, Joe is a critical user, who tends to rate\n",
        "0.3 stars lower than the average.\n",
        "\n",
        "Thus, the estimate for Titanic’s rating by Joe would be 3.9 stars (3.7 + 0.5 - 0.3).\n",
        "\n",
        "As you can see, there are three factors behind determining Joe's rating of Titanic:-\n",
        "- **Average rating in the Netflix eco-system**\n",
        " - An average of all movies present in Netflix, let this be $\\mu$\n",
        "- **Average rating of the specific movie**\n",
        " - Movies like Titanic, Godfather have a very high ratings, let this be $b_i$, i.e. bias of the movie\n",
        "- Average ratings given by the specific user\n",
        " - Some users tend to be very critical in giving ratings, while some are very generous, let this be $b_u$, i.e. bias of the user\n",
        "\n",
        "Hence, the rating for a movie i by user u becomes:\n",
        "\n",
        "$r_{ui} = \\mu + b_u + b_i + q_i^T.p_u$\n",
        "\n",
        "<br>\n",
        "\n",
        "Now, instead of freezing the $\\mu, b_u, b_i$ values, let's make them a part of our optimization problem, whilst treating them as **scalars**.\n",
        "\n",
        "So from a regression standpoint,\n",
        "- $q_i^T.p_u$: becomes a quadratic term\n",
        "- $b_u, b_i$: become the linear terms\n",
        "- $\\mu$: constant term\n",
        "\n",
        "$p_u, q_i, \\mu, b_u, b_i$ act as parameters now. Therefore the optimization problem becomes:-\n",
        "\n",
        "$min_{p_u, q_i, \\mu, b_u, b_i} Σ_{u, i} (r_{ui} - \\mu - b_u - b_i - q_i^T.p_u)^2 + λ*Σ( ||q_i||^2 + ||p_u||^2 + μ^2 + b_u^2 + b_i^2)$\n"
      ],
      "metadata": {
        "id": "KoRxjb8DkRjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Temporal Dynamics\n",
        "\n",
        "> **NOTE:**\n",
        "- There's another technique called **implicit feedback** that we are not showing, as this temporal dynamic shows more improvement, and there is time constraint.\n",
        "\n",
        "Recall that our models earlier have had the disadvantage that they do not capture any **change in user's preferences over time.**\n",
        "\n",
        "The idea to try to capture this is called **Temporal dynamics**. This significantly improved the recommendations, and reduced RMSE.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How can we capture change in user's preferences over time?**\n",
        "\n",
        "By treating rating of a movie $i$ by a user $u$ as a **function of time**. Hence our formulation for ratings become:\n",
        "\n",
        "$r_{ui}(t) = \\mu + b_u(t) + b_i(t) + q_i^T.p_u(t)$\n",
        "\n",
        "This makes sense\n",
        "- The global value of average ($\\mu$) is independant of time.\n",
        "- The user 's ratings bias depends on time ($b_u$)\n",
        " - User's taste and preferences can and will change over time.\n",
        " - A user watching tom and jerry today will be watching altogether different kinds of movies now, like James Bond.\n",
        "- The items ratings bias depends on time ($b_i$)\n",
        " - There might be older movies that people are rediscovering, and giving them higher ratings\n",
        "- Similarly, the user's and item's vectors ($q_i^T.p_u$) representation would also be changing with time.\n",
        "\n",
        "The idea was that instead of modelling these parameters as a constant value across ten years worth of data, we model them as time series based models, since they can change over time.\n",
        "\n",
        "We will study about time series in depth in the next few lectures.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How would we train such a model?**\n",
        "\n",
        "In order to train this model, we would consider data in chunks of time, as opposed to training the 10 years worth of data altogether.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1GSC1C4-rwgY9gJHMKv2_NyNGV9gALSeq)\n",
        "\n"
      ],
      "metadata": {
        "id": "9C1WIqDLTsO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization of performance of different models (RMSE) vs. No of parameters (d)\n",
        "\n",
        "`d`: Dimensions of the latent space\n",
        " - Dimensions of $q_i$ and $p_u$\n",
        " - Obviously, as d increases, the number of parameters increase.\n",
        "\n",
        "**Observations from the visualization:-**\n",
        "- The plain model does not improve beyond a certain point, as the number of parameters increase.\n",
        "- On just adding the biases, we can see improvement.\n",
        "- There's another technique called implicit feedback that shows big improvement.\n",
        "- **Temporal dynamics** showed the most significant improvemenet in RMSE score.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1oouKmbw-NgGGvOBiTBFev670gHP2WkmI)"
      ],
      "metadata": {
        "id": "7t94hRcMaPe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Q. Imagine you are working at YouTube, how will you build a time-sensitive recommendation system?\n",
        "\n",
        "- **Idea 1:** We can update the parameters ($b_u, b_i, q_i, p_u$) periodically.\n",
        " - This is a valid approach, we can do this.\n",
        "\n",
        "- **Idea 2:** Give more weightage to more recent data.\n",
        " - Formulation becomes: $min_{p_u, q_i, \\mu, b_u, b_i} Σ_{u, i} w_{ui}(r_{ui} - \\mu - b_u - b_i - q_i^T.p_u)^2 + λ*Σ( ||q_i||^2 + ||p_u||^2 + μ^2 + b_u^2 + b_i^2)$\n",
        " - Here, $w_{ui}$ represents a matrix containing weights. It is constructed in such a way that more recent the value, more the weightage.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1sRoouYZRUpCbr7Bp7545WG8yShBySmDo)\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1aHBAYZeM8djkJB0ROQCzvvDyRdNSOU_i)\n",
        "\n"
      ],
      "metadata": {
        "id": "OSPbHSIFdcJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Let's see how it actually works**\n",
        "\n",
        "Suppose the user is watching a video $v_{100}$, then the recommendations seen there are:-\n",
        "- Some recommendations are  **purely recency based** similarity\n",
        " - This can be achieved by finding **K Nearest Neighbours** to video $v_{100}$. This has to be very fast.\n",
        "\n",
        "- There will be some videos based on **historical taste**.\n",
        " - Everything is precomputed offline (**nightly**) as a **batch process** (at regular intervals), and more importance is given to recent data (watch history). This can be found purely through **Matrix Factorization**.\n",
        " - So, every night, we can perform matrix factorization, giving more importance to the d-dimensional item vectors and the item bias ($q_i, b_i$).\n",
        " - Once MF is performed **nightly**, we can easily take the top 100 videos that are good to recommend based on the user's history."
      ],
      "metadata": {
        "id": "qW0-4-VMcsyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## MF: Feature Engineering"
      ],
      "metadata": {
        "id": "yEHbU0VRhwEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For entities"
      ],
      "metadata": {
        "id": "PGq7OnLJK1We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Consider the Netflix usecase. We have 2 entities:-\n",
        "- user\n",
        "- movie\n",
        "\n",
        "Recall that in our MF based recommendation system problem, we care the most about getting d-dimensional user and item vectors ($q_i$'s and $p_u$'s), as using them, we can complete our matrix $r$, and then give out recommendations.\n",
        "\n",
        "So, essentially, MF is doing feature engineering.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How is MF doing feature engineering?**\n",
        "\n",
        "In an  **implicit** manner.\n",
        "\n",
        "Because using MF we're getting $p_u$'s which are very sensible features for a user, because similar users would have a very similar vector.\n",
        "\n",
        "Similarly, items of similar nature would also have very similar item vectors (features), i.e. $q_i$'s.\n",
        "\n",
        "Now, using these features ($q_i$'s and $p_u$'s), we can train a classification, regression, visualization, clustering models, etc. We can use them as we want.\n",
        "\n",
        "We're literally getting user and item features out of thin air!\n",
        "\n",
        "![image](https://docs.google.com/uc?id=17fkqOMDnPrlXOWGRg6MJmrQgIVk8PThQ)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What information do these features store?**\n",
        "\n",
        "Consider the user vector $p_u$. Traditionally, when we do FE, we represent features like\n",
        "- what country user is from\n",
        "- What is the pin code\n",
        "... and so on\n",
        "\n",
        "These features have a certain meaning, or interpretability about them.\n",
        "\n",
        "But in case of FE using MF, we're getting d-dimensional representation of the user, and these do not hold any kind of interpretable information, as they are algorithmically engineered.\n",
        "\n",
        "Though there is **no interpretability**, we can visualize these vectors using **UMAP/t-SNE** techniques\n",
        "\n",
        "\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1gn6CLDXajnfZqwMc0nd2oylrzoGySz3l)"
      ],
      "metadata": {
        "id": "LKcYPXG1h0vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Example:-**\n",
        "\n",
        "Consider the world of e-commerce.\n",
        "\n",
        "The **recommender systems team** can compute user and item vectors $q_i$'s and $p_u$'s,  using their data matrix, and store it in a **feature store**.\n",
        "\n",
        "This data could be sales data/ purchase data/ number of times page visited data, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What is a feature store?**\n",
        "\n",
        "Majorly used in big companies, feature store is a **key-value storing** of data.\n",
        "\n",
        "Using this, when given a **userID**, we can instantly get the d-dimensional representation of the user $p_u$.\n",
        "\n",
        "This can also be used to get item vectors $p_i$, given the **itemID**."
      ],
      "metadata": {
        "id": "g1nB6uuLGBwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose somebody searches for \"bluetooth earphones\".\n",
        "\n",
        "It is the task of the **search team**, to take all of the item vectors that were built by the RecSys team, and use it to improve the search results.\n",
        "\n",
        "<bR>\n",
        "\n",
        "> **Q. How can the Search team improve their search results?**\n",
        "\n",
        "Suppose there is a product, that was given as the first, most relevant result.\n",
        "\n",
        "Now, the search team can just find all the similar items to this product, and return those using K Nearest Neighbour\n",
        "\n",
        "![image](https://docs.google.com/uc?id=13B0e4TTRNDLFp7OhHhcRlC_RmKn25DlI)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Can we use this data in some other context/usecase?**\n",
        "\n",
        "Yes.\n",
        "\n",
        "Essentially, using MF, we now have a d-dimensional representation for different users and items. We can use this data as we see fit.\n",
        "\n",
        "For example,\n",
        "- Consider a user is browsing a site, and we wish to show an ad.\n",
        "- For our ad model, we essentially have to calculate the **probability of user clicking on the ad**, given some information about the user, the ad/item, the website visited, etc, i.e. $P(click|user, ad, website, ...)$\n",
        "- Naturally, we need some user level, and item level features to build this model. Here, we can use the d-dimensional vectors we obtained for the user and item as features.\n",
        "- Even though the goal here was not to recommend something, but to calculate probability, we were able to utilise the features obtained using MF.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1jWi12W0KROwmLruO9X0060VKU1_VWZNY)\n"
      ],
      "metadata": {
        "id": "s5QYr46wH6O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### For text data\n",
        "\n"
      ],
      "metadata": {
        "id": "oncPvWeWK5u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a large corpus of text, from Wikipedia dataset, consisting of all articles on Wikipedia.\n",
        "\n",
        "\n",
        "We need to get a d-dimensional representation for words using MF, such that it need not be interpretable, but it should be sensible.\n",
        "\n",
        "**NOTE:**\n",
        "- By sensible we mean that if we visualize them, similar words should group together.\n",
        "- We want a d-dimensional representation for a specific word, and not the entire document. We're not talking about BoW.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1J6_dGdMjsSLIM1E8-FV0n6gHoVvxfVVo)\n"
      ],
      "metadata": {
        "id": "bZSENd-1K8Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Q. How can we represent the data?**\n",
        "\n",
        "Let's build a matrix $A$ where each row represents an article, and each column represents a word.\n",
        "\n",
        "Let the total number of articles/document be $n$ and number of words be $m$. Hence the dimensions of this matrix become: `n x m`.\n",
        "\n",
        "If a document i, contains a word j, then $A_{ij}$ stores the count/frequency of that word. Hence if word j doesn't appear in document i, $A_{ij}=0$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=170n1nNfGXrLuKq-lrlk4Y3LpB803LfPY)\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How can we utilise this data representation to get d-dimensional representation for a word j?**\n",
        "\n",
        "We have a matrix containing information about the frequency of different words in different articles.\n",
        "\n",
        "If we decompose this matrix as product of two matrices, we can have a d-dimensional representation of both articles and words.\n",
        "\n",
        "$A_{nxm} = B_{nxd}.C_{dxm}$\n",
        "\n",
        "\n",
        "Hence, the **jth column** of matrix $C$ becomes the d-dimensional representation of word $w_j$.\n",
        "\n",
        "Similarly, the **ith row** of matrix $B$ becomes the d-dimensional representation of document $d_i$.\n",
        "\n",
        "These vectors will be **dense**.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1sR-9a5qy2hiDun-8uQLI55h2Z1XlBWX8)\n",
        "\n"
      ],
      "metadata": {
        "id": "m49pvVO3z28g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Q. How can we use these representations of documents and words?**\n",
        "\n",
        "We can use them as we see fit. Some examples:-\n",
        "- Since we have document vectors, we can find **similar documents**.\n",
        " - This is useful for websites like Kindle, Google news, blog sites, etc.\n",
        " - Though MF can get this job done, most of these sites would be using Deep Learning in practice.\n",
        "- Similarly, given a word $w_j$, we can find all the **similar words**.\n",
        " - Used in product searches, grammerly, etc.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1a8nq6-t-0eaCoKepKtI5s9MBHs3GmCm2)\n"
      ],
      "metadata": {
        "id": "EJ7NOquf37Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Co-occurence matrix approach**\n",
        "\n",
        "There's an alternate way to to do FE for text data using MF. Suppose we have $m$ words.\n",
        "\n",
        "Let's build a square matrix $X$ with dimensions are: `m x m`.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What does $X_{ij}$ represent?**\n",
        "\n",
        "$X_{ij}$ represents the number of times a word $w_j$ **occurs in the context of** / vicinity of another word $w_i$.\n",
        "\n",
        "This matrix $X$ is called the **co-occurence matrix**.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1gSVpw3OP4BggvOSpo7hEAsK2UT1vBqD_)\n"
      ],
      "metadata": {
        "id": "gqmHniGD6KqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Q. What do we mean by a word $w_j$ lying \"in the context of\" word $w_i$?**\n",
        "\n",
        "Let there be a sentence such that it contains words in the following order: $w_6 w_j w_1 w_2 w_i w_1 w_3 w_4$\n",
        "\n",
        "We can say that word $w_j$ lying \"in the context of\" word $w_i$ if they are within a neighborhood (a short distance).\n",
        "\n",
        "Let us assume that neighbourhood distance is 10. Then, if $w_j$ and $w_i$ are separated by less than or equal to 10 words, we can say that are in the neighbourhood of each other.\n",
        "\n",
        "For example:\n",
        "- Words like \"Principal\" and \"Analysis\" have very different meaning.\n",
        "- But if they are in context of each other, then we can understand that perhaps the document is referring to the concept of PCA, a technique in ML.\n",
        "- Clearly, two words with very different meaning, when appearing in a context, can have a different meaning altogether.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1X_wpmjewSi_ysz7pwh3VX46MjCkpFmlz)\n",
        "\n",
        "**NOTE:**\n",
        "- Naturally, $X$ would be **symmetric**.\n",
        "\n",
        "<br>\n",
        "\n",
        "Now, since $X$ is square and symmetric, we can decompose this co-occurence matrix using PCA.\n",
        "\n",
        "But for simplicity, let's consider deomposing using MF only. So we get,\n",
        "\n",
        "$X_{mxm} = B_{mxd}.C_{dxm}$\n",
        "\n",
        "On decomposing, we get two matrices: `B and C`.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. What do the matrices `B and C` represent?**\n",
        "\n",
        "$X$ is a matrix where both columns and rows represent words.\n",
        "\n",
        "Since we get `B and C` upon decomposing $X$, both of these matrices are d-dimensional vectors of words only.\n",
        "\n",
        "Here, for every word, we have two d-dimensional representations!"
      ],
      "metadata": {
        "id": "csUdoYB29Vaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Truncated SVD**\n",
        "\n",
        "Suppose given our co-occurence matrix, we decompose it using SVD as:\n",
        "\n",
        "$X_{mxm} = U_{mxm}.Σ_{mxm}.V_{mxm}^T$\n",
        "\n",
        "where,\n",
        "- $U_{mxm}$: Left singular vectors; Eigen vectors of $X.X^T = S'$\n",
        "- $Σ_{mxm}$: Diagonal matrix conntaining singular values\n",
        "- $V_{mxm}^T$: Right singular vectors; Eigen vectors of $X^T.X = S$\n",
        "\n",
        "A property of the singular values in $Σ$ is that: $s_1>=s_2>=s_3>=...>=s_m$.\n",
        "\n",
        "Keep in mind that m  (no of words) is a very large number. So naturally, $s_m$ has a very very small value.\n",
        "\n",
        "TODO: Scribble\n",
        "\n",
        "So we end up doing a lot of computation, and it doesn't have that much of an impact either.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How can we solve this problem?**\n",
        "\n",
        "We can choose a value $k$, such that, beyond $s_k$ the singular values are so small that considering them is not giving us any advantage.\n",
        "\n",
        "We only consider the top $k$ singular values in our computation, and ignore the singular values from $s_{k+1}$ to $s_m$.\n",
        "\n",
        "This lessens the computational load as well, and we're not losing any data either.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. Since the dimensions of $Σ$ is `mxm`, how so we choose the top k singular values?**\n",
        "\n",
        "We change the dimension of $Σ$ from `m x m` to `k x k`, thereby getting rid of the remaining matrix. Let this matrix be $\\hat{Σ}_{kxk}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. If the dimensions of $Σ$ are changed, wouldn't that make the decomposition equation invalid?**\n",
        "\n",
        "Yes, it would. Hence we need to change the dimensions of $U$ and $V^T$ matrices also.\n",
        "\n",
        "\n",
        "We only consider the first `k` columns in $U$, and discard the remaining $m-k$ columns. This becomes $\\hat{U}_{m x k}$. It denotes the top left `k` singular vectors.\n",
        "\n",
        "Similarly, we only consider the first `k` rows in $V^T$. This becomes $\\hat{V^T}_{k x m}$. It denotes the top right `k` singular vectors.\n",
        "\n",
        "Hence, the new decomposition formulation looks like:\n",
        "$X_{mxm} = \\hat{U}_{mxk}.\\hat{Σ}_{kxk}.\\hat{V^T}_{kxm}$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1XVGWN3rrMEXpk0csFO-WphjhYj1AhCNG)\n",
        "\n",
        "<br>\n",
        "\n",
        "This technique of considering only the top `k` singular values is called **Truncated SVD**.\n",
        "\n",
        "Where `k` becomes a **hyperparameter**.\n",
        "\n"
      ],
      "metadata": {
        "id": "gZPB3h6_ZOI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Truncated SVD, now we have decomposed the data matrix X.\n",
        "\n",
        "> **Q. Now how do we get the word vector for a word $w_j$?**\n",
        "\n",
        "The dimensions of $\\hat{U}$ are `n x k`.\n",
        "\n",
        "The word vector of a word $w_j$ would be the jth row of $\\hat{U}$ matrix.\n",
        "\n",
        "TODO: scribble\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "- In practice, even applying Truncated SVD on the co-occurrence matrix can be very computationally expensive, due to the large number of words (m).\n",
        "- One hack to handle this is that instead of using all the words, we use the top 'n' words, which can be picked using the IDF scores in TF-IDF."
      ],
      "metadata": {
        "id": "ZX36UlZrl4F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Images: Eigen Faces"
      ],
      "metadata": {
        "id": "PpCatVMWLbdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we can use the conept of MF to do **face recognition**.\n",
        "\n",
        "> **Q. How are images represented?**\n",
        "\n",
        "Given a 2D image, it'll be given as a 2D matrix with $r$ rows and $c$ columns. Let the images be present in the shape of $64 x 64$ square.\n",
        "\n",
        "This means that total number of cells is $r*c = 64*64 = 4096$.\n",
        "\n",
        "We take the rows from this matrix, and store them one after the other as a vector of dimension $r*c$. This is called **row major order**.\n",
        "\n",
        "We **flatten** the pixel matrix.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=12neQnnIwdCvjmq27G30On0xWkT2rLRKG)\n",
        "\n",
        "<br>\n",
        "\n",
        "Suppose we have 400 images's raw pixel data: $n=400$\n",
        "\n",
        "We create a data matrix $X$, such that each row of $X$ represents an image. And we store $n=400$ such images, and have $d$ columns, such that $d=r*c=4096$.\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1XCwxx4RHQY4Jaw23oIdjaf3RvOFhD7er)\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "LoUprjLRLdzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for this data matrix, though we can decompose it using any technique, let's do it using **PCA**.\n",
        "\n",
        "This workflow of using **PCA** to do face recognition is called **eigen faces**.\n",
        "\n",
        "This technique was used earlier in the past, and is no longer used. Now it has been replaced by **Convolutional Neural Networks (CNNs)** that we will study about in the NN module.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How do we apply PCA on the data matrix $X$?**\n",
        "\n",
        "Since PCA only works for square matrices, we need to first calculate the **co-variance matrix** $S_{dxd}$, and then we apply PCA on $S$.\n",
        "\n",
        "$S_{dxd} = W_{dxd}.∧_{dxd}.W_{dxd}^T$\n",
        "\n",
        "![image](https://docs.google.com/uc?id=1nobLGYka0nqr5iQY5FGxXBqMq2thOi-G)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Let us now reduce the dimensionality from `d` to `k`, by picking the top `k` eigenvectors and eigenvalues ($k<d$) in the $W$ matrix.\n",
        "\n",
        "Let's call this matrix $W_k$ and it's dimensions become `d x k`.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Q. How can we get a k-dimensional representation for images?**\n",
        "\n",
        "Now in order to get a k-dimensional representation of images, we need to multiply\n",
        "\n",
        "$X_{nxd}.W_{k_{dxk}} = \\tilde{A}_{nxk}$\n",
        "\n",
        "where $\\tilde{A}$ gives a k-dimensional vector for each of the given image.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Now that we have $\\tilde{A}$, we can **un-flatten** this into $64x64$ format, to get back an image.\n"
      ],
      "metadata": {
        "id": "A25AV3dppDZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "i1UMnEHL08mW"
      }
    }
  ]
}